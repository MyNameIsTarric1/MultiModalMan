\section{Aim and Scope}

\noindent
\textit{MultiModalMan} pursues a twofold mission: to illustrate how multimodal interfaces lower accessibility barriers and to turn that insight into an engaging, language-learning game.  Traditional mouse-and-keyboard designs routinely sideline entire user groups.  Blind players cannot perceive graphical feedback; Deaf players can read on-screen text but gain little from audio narration; users with motor impairments may struggle with precise key presses.  By blending speech, gesture and text input, our project offers redundant—and therefore more inclusive—paths through the game.

\subsection{Accessibility Objectives}

\begin{description}[leftmargin=1.2em,style=nextline]
  \item[\textbf{Multiple channels}] Voice, hand gestures and on-screen buttons are interchangeable so that no single sense or motor skill becomes a blocker.
  \item[\textbf{Low reading load}] Visual icons and spoken prompts replace verbose text wherever possible.
  \item[\textbf{Complementarity}] When one modality fails (e.g.\ speech in a noisy room) another can take over without interrupting play.
\end{description}

\subsection{Educational \& Inclusive Goals}

\begin{description}[leftmargin=1.2em,style=nextline]
  \item[\textbf{Broad reach}] Children, language learners and users with motor or speech constraints can all follow the same storyline.
  \item[\textbf{Multisensory learning}] Players reinforce spelling via simultaneous visual, auditory and kinaesthetic cues.
\end{description}

\subsection{Technical Targets}

\begin{description}[leftmargin=1.2em,style=nextline]
  \item[\textbf{Accurate input}] Real-time recognition of spoken letters and hand-drawn glyphs.
  \item[\textbf{Context-aware logic}] An AI agent that interprets free-form utterances and drives adaptive hints.
  \item[\textbf{Unified control}] A central state manager that keeps the three modalities synchronised and responsive.
\end{description}
