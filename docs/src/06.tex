\section{System Architecture}

\textbf{MultiModalMan} is implemented as a web application using Python and the Flet framework. The architecture is designed to run on Windows, Linux, and macOS systems, requiring only a standard webcam and microphone for full multimodal functionality.

\subsection*{High Level Architecture}
The system follows a layered architecture pattern with clear separation between the presentation layer (frontend), business logic (controller), and specialized processing services (backend). This design ensures modularity, maintainability, and enables independent development of different components.

The application consists of three main architectural layers:

\begin{description}
\item[\textbf{Frontend Layer}] Web based user interface built with Flet, handling all user interactions and visual feedback
\item[\textbf{Controller Layer}] AI powered game agent that manages game flow and natural language processing
\item[\textbf{Backend Layer}] Computer vision and machine learning models for gesture recognition
\end{description}

\subsection*{Module Structure}
The project is organized into three primary modules, each serving distinct architectural roles:

\subsubsection*{\texttt{frontend/}}
The frontend module implements the web based user interface using the Flet framework. It contains the complete presentation layer and game state management:

\begin{description}
\item[\texttt{main.py}] Application entry point that initializes the web interface and coordinates between components
\item[\texttt{src/app/}] Core application logic including:
  \begin{itemize}
    \item \texttt{game\_logic.py}: \texttt{HangmanGame} class managing game rules, word tracking, and win/loss conditions
    \item \texttt{state\_manager.py}: \texttt{GameStateManager} singleton implementing the observer pattern for real time UI updates
  \end{itemize}
\item[\texttt{src/components/}] Modular UI components:
  \begin{itemize}
    \item \texttt{game\_panel.py}: Main game display with word visualization and controls
    \item \texttt{media\_controls.py}: Multimodal input interface (voice, gesture, chat)
    \item \texttt{hand\_drawing\_recognition.py}: Computer vision integration for gesture input
    \item \texttt{display.py}, \texttt{layout.py}: Responsive UI layout and visual components
  \end{itemize}
\end{description}

\subsubsection*{\texttt{controller/}}
The controller module houses the AI powered game agent that serves as an intelligent intermediary between user inputs and game actions:

\begin{description}
\item[\texttt{agent.py}] OpenAI conversational agent that:
  \begin{itemize}
    \item Interprets natural language commands and converts them to game actions
    \item Maintains conversation context and provides adaptive hints
    \item Manages game flow through structured function calls
    \item Integrates with the frontend's \texttt{GameStateManager} for state synchronization
  \end{itemize}
\end{description}

The agent uses a singleton pattern to ensure consistency between frontend and controller state management, enabling seamless communication between the AI logic and the User Interface.

\subsubsection*{\texttt{backend/}}
The backend module provides specialized machine learning capabilities for gesture recognition:

\begin{description}
\item[\texttt{models/}] Pre trained neural network models:
  \begin{itemize}
    \item \texttt{letter\_recognition.h5}: CNN model trained on EMNIST dataset for hand drawn letter classification
  \end{itemize}
\item[\texttt{src/}] Computer vision processing pipeline:
  \begin{itemize}
    \item \texttt{hand\_model.py}: \texttt{HandModel} class for loading and running inference on hand drawn letters
    \item \texttt{tracker.py}: Real time hand tracking and gesture detection using OpenCV
  \end{itemize}
\end{description}

\subsection*{Data Flow Architecture}

The system implements a centralized state management pattern where all user inputs regardless of modality flow through the \texttt{GameStateManager}. This ensures consistent game state and enables seamless switching between input methods:

\begin{enumerate}
\item \textbf{Input Processing}: Voice (speech recognition), gestures (CNN inference), or text input are processed by their respective handlers
\item \textbf{State Management}: All inputs are channeled through the \texttt{GameStateManager}, which validates actions and updates game state
\item \textbf{Observer Notification}: UI components are automatically updated via the observer pattern when state changes occur
\item \textbf{AI Integration}: The OpenAI agent maintains parallel state synchronization for natural language processing and adaptive responses
\end{enumerate}

This architecture ensures that users can switch fluidly between input modalities without losing game context or experiencing state inconsistencies.

\subsection*{Deployment Architecture}

\textit{MultiModalMan} is deployed as a web application that runs locally and serves a browser-based interface. The deployment model supports rapid development and cross-platform compatibility:

\begin{description}
\item[\textbf{Local Server}] The Flet framework creates a local web server that hosts the application interface
\item[\textbf{Browser Client}] Users interact through any modern web browser, requiring no additional software installation
\item[\textbf{External Services}] The application integrates with external APIs:
  \begin{itemize}
    \item OpenAI API for natural language processing and conversational AI
    \item Google Speech Recognition API for voice input processing
  \end{itemize}
\end{description}
